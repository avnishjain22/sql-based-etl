apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: test-iam-
  namespace: spark
  # need the name if use kubectl to submit the job
  # name: test-argo-wrfw
spec:
  serviceAccountName: arcjob
  entrypoint: testiam
  # delete all pods as soon as they complete
  # podGC: 
  #   strategy: OnPodCompletion
  # must complete in 8h
  activeDeadlineSeconds: 28800
  # keep workflows for 12h
  ttlStrategy:
    secondsAfterCompletion: 43200
  templates:
  - name: testiam
    inputs:
      parameters:
      - name: jobId
        value: testiamforsvcacct
      - name: configUri
        value: "s3a://testtestmelody/app-code/job/auto_job.ipynb"
      - name: image
        value: ghcr.io/tripl-ai/arc:arc_3.5.2_spark_3.0.1_scala_2.12_hadoop_3.2.0_1.0.0
      - name: executorInstances
        value: "2"
      - name: executorCores
        value: "1"
      - name: executorMemory
        value: "5"
      - name: sparkConf
        value: ""
      - name: tags
        value: ""
      - name: parameters
        value: "--ETL_CONF_DATALAKE_LOC=testtestmelody"
      - name: pullPolicy
        value: IfNotPresent
      # to exec each stages at a jupyter notebook, we can controle it by matching the environment. Some stages may not required in prod env.   
      - name: environment
        value: test  
    metadata:
      labels:
          app: spark
          workflowId: "{{workflow.uid}}"
    script:
      resources:
        requests:
          cpu: "1"
          memory: "1Gi"
      image: "{{inputs.parameters.image}}"
      command: ["/bin/sh"]
      source: |
        # verbose logging
        set -ex

        # print current hostname and ip
        hostname
        hostname -I

        # submit job
        bin/spark-submit \
        --master k8s://kubernetes.default.svc:443 \
        --deploy-mode client \
        --class ai.tripl.arc.ARC \
        --name arc \
        --conf spark.authenticate=true \
        --conf spark.driver.extraJavaOptions="-XX:+UseG1GC" \
        --conf spark.driver.host=$(hostname -I)  \
        --conf spark.driver.memory=921m \
        --conf spark.driver.pod.name=$(hostname) \
        --conf spark.executor.cores={{inputs.parameters.executorCores}} \
        --conf spark.executor.extraJavaOptions="-XX:+UseG1GC" \
        --conf spark.executor.instances={{inputs.parameters.executorInstances}} \
        --conf spark.executor.memory={{inputs.parameters.executorMemory}}G \
        --conf spark.io.encryption.enabled=true \
        --conf spark.kubernetes.authenticate.caCertFile=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName={{workflow.serviceAccountName}} \
        --conf spark.kubernetes.authenticate.oauthTokenFile=/var/run/secrets/kubernetes.io/serviceaccount/token \
        --conf spark.kubernetes.container.image.pullPolicy={{inputs.parameters.pullPolicy}} \
        --conf spark.kubernetes.container.image={{inputs.parameters.image}} \
        --conf spark.kubernetes.driver.limit.cores=1 \
        --conf spark.kubernetes.driver.pod.name=$(hostname) \
        --conf spark.kubernetes.executor.label.workflowId={{workflow.uid}} \
        --conf spark.kubernetes.executor.limit.cores={{inputs.parameters.executorCores}} \
        --conf spark.kubernetes.executor.podNamePrefix=$(hostname)-spark \
        --conf spark.kubernetes.executor.request.cores={{inputs.parameters.executorCores}} \
        --conf spark.kubernetes.local.dirs.tmpfs=false \
        --conf spark.kubernetes.namespace={{workflow.namespace}} \
        --conf spark.network.crypto.enabled=true \
        --conf spark.ui.enabled=true \
        {{inputs.parameters.sparkConf}} \
        local:///opt/spark/jars/arc.jar \
        --etl.config.uri={{inputs.parameters.configUri}} \
        --etl.config.environment={{inputs.parameters.environment}} \
        --etl.config.ignoreEnvironments=true \
        --etl.config.job.id={{inputs.parameters.jobId}} \
        --etl.config.tags="service=arc workflowId={{workflow.uid}} pod={{pod.name}} serviceAccount={{workflow.serviceAccountName}} namespace={{workflow.namespace}} {{inputs.parameters.tags}}" \
        --ETL_CONF_EPOCH=$(date '+%s') --ETL_CONF_CURRENT_TIMESTAMP=$(date -u '+%Y-%m-%d %H:%M:%S') \
        {{inputs.parameters.parameters}}
