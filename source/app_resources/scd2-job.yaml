apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  # need it if using ArgoCLI
  # generateName: scd2-job-
  namespace: spark
  ## need it if use kubectl
  name: scd2-job
spec:
  serviceAccountName: arcjob
  entrypoint: scd2-load
  ## must complete in 8h
  activeDeadlineSeconds: 28800
  ttlStrategy:
    # keep workflows for 12h
    secondsAfterCompletion: 43200
    SecondsAfterFailure: 43200
  templates:
  - name: scd2-load
    dag:
      tasks:
        - name: delta-load-SCD2
          templateRef:
            name: spark-template
            template: mediumJob
            clusterScope: true 
          arguments:
            parameters:
            - name: jobId
              value: scd2 
            - name: image
              value: 720560070661.dkr.ecr.us-west-2.amazonaws.com/arc:latest
            ## in the sample jupyter notebook, all stages will be run in test and prod only.
            - name: environment
              value: test   
            - name: tags
              value: "project=sqlbasedetl, owner=myang, costcenter=66666"     
            # Refer to the jupyter notebook
            - name: configUri
              value: "s3a://sparkoneks-codebucket9d8d5b72-1fdsh6fsy4xol/job/scd2_job.ipynb"
            - name: parameters
              value: "--ETL_CONF_DATALAKE_LOC=sparkoneks-codebucket9d8d5b72-1fdsh6fsy4xol"
            - name: sparkConf
              value: "--conf spark.kubernetes.driver.podTemplateFile='driver-pod-template.yaml' --conf spark.kubernetes.executor.podTemplateFile='executor-pod-template.yaml'"   
