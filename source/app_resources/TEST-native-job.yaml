apiVersion: batch/v1
kind: Job
metadata:
  name: test-spot
  namespace: spark
spec:
  template:
    spec:
      # serviceAccountName: nativejob
      containers:
      - name: test-spot-executor
        image: 720560070661.dkr.ecr.us-west-2.amazonaws.com/arc:latest
        command: [
        "/bin/sh",
        "-c",
        "/opt/spark/bin/spark-submit \
        --master k8s://kubernetes.default.svc:443 \
        --deploy-mode client \
        --name 'Word Count' \  
        --conf spark.executor.instances=20 \
        --conf spark.kubernetes.allocation.batch.size=10 \ 
        --conf spark.kubernetes.driver.request.cores=2 \
        --conf spark.kubernetes.driver.limit.cores=4 \
        --conf spark.executor.memory=8g \
        --conf spark.kubernetes.executor.request.cores=2 \
        --conf spark.kubernetes.executor.limit.cores=4 \
        --conf spark.sql.shuffle.partitions=60 \
        --conf spark.kubernetes.container.image=720560070661.dkr.ecr.us-west-2.amazonaws.com/arc:latest \
        --conf spark.kubernetes.container.image.pullPolicy=Always \
        --conf spark.kubernetes.driver.podTemplateFile='driver-pod-template.yaml' \
        --conf spark.kubernetes.executor.podTemplateFile='executor-pod-template.yaml' \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName=nativejob \
        --conf spark.kubernetes.namespace=spark \
        --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
        --conf spark.hadoop.fs.s3a.fast.upload=true \
        --conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.WebIdentityTokenCredentialsProvider \
        wordcount.py \
        \"s3a://${DATALAKE_BUCKET}/output\""
        ]
      serviceAccountName: nativejob
      restartPolicy: Never 
  backoffLimit: 5      
