apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: nyctaxi-job-
  namespace: spark
  ## need it if use kubectl to submit the job
  # name: nyctaxi-job
spec:
  serviceAccountName: arcjob
  entrypoint: nyctaxi
  # must complete in 8h (28,800 seconds)
  activeDeadlineSeconds: 28800
  # keep workflows for 12h (43,200 seconds)
  ttlStrategy:
    secondsAfterCompletion: 43200
    SecondsAfterFailure: 43200
  templates:
  - name: nyctaxi
    dag:
      tasks:
        - name: step1-query
          templateRef:
            name: spark-template
            template: smallJob
            clusterScope: true   
          arguments:
            parameters:
            - name: jobId
              value: nyctaxi 
            - name: image
              value: ghcr.io/tripl-ai/arc:arc_3.5.2_spark_3.0.1_scala_2.12_hadoop_3.2.0_1.0.0  
            # in a jupyter notebook, each stage associates to env, eg.test & prod. Could skip a stage when env mismatches 
            - name: environment
              value: test    
            - name: tags
              value: "project=sqlbasedetl, owner=myang, costcenter=66666"  
            - name: configUri
              value: https://raw.githubusercontent.com/tripl-ai/arc-starter/master/examples/kubernetes/nyctaxi.ipynb
            - name: parameters
              value: "--ETL_CONF_DATA_URL=s3a://nyc-tlc/trip*data --ETL_CONF_JOB_URL=https://raw.githubusercontent.com/tripl-ai/arc-starter/master/examples/kubernetes"